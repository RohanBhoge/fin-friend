{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7162dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programs\\01_Projects\\Projects\\04_FinFriend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss  # This will now use the GPU version\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# --- LangChain Imports for RAG ---\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347a5fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\bhoge\\AppData\\Local\\Temp\\ipykernel_26704\\2125688349.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\", device=0, max_new_tokens=150)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47cabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('./manageMoney.txt',encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77cf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller, manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec101b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models to GPU... This might take a few minutes the first time.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models to GPU... This might take a few minutes the first time.\")\n",
    "\n",
    "# Load the embedding model directly to the GPU\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "\n",
    "# Load the Language Model (LLM) and its tokenizer to the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9da81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = \"google/flan-t5-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c53333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "# Load model onto the GPU (device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f66522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_id).to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a852f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully! ✅\n"
     ]
    }
   ],
   "source": [
    "# Create a Hugging Face pipeline that runs on the GPU\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    device=0  # device=0 corresponds to the first GPU\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"Models loaded successfully! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d85ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "loader = TextLoader('./manageMoney.txt',encoding='utf-8',)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3470021",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \",\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "all_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930d1fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 112 text chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Created {len(all_chunks)} text chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e04455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings for all chunks...\")\n",
    "chunk_texts = [chunk.page_content for chunk in all_chunks]\n",
    "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True)\n",
    "print(\"Embeddings created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8681d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building GPU-accelerated FAISS index...\n"
     ]
    }
   ],
   "source": [
    "# --- Build GPU-ACCELERATED FAISS INDEX ---\n",
    "print(\"Building GPU-accelerated FAISS index...\")\n",
    "d = embeddings.shape[1]  # Dimension of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becbaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a standard CPU index (this is a placeholder)\n",
    "index = faiss.IndexFlatL2(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d47f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a GPU resource object to manage GPU memory\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac7df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU index is ready! Indexed 112 vectors. ✅\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU index is ready! Indexed {index.ntotal} vectors. ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58caf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep your 'retrieve_and_format_docs' function as it is)\n",
    "def retrieve_and_format_docs(query_text: str, k: int = 4) -> str:\n",
    "    \"\"\"Helper function to retrieve docs from FAISS and format them.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query_text])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_docs = [all_chunks[i] for i in indices[0]]\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Content: {doc.page_content}\\nSource: {doc.metadata.get('source', 'N/A')}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3212989",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95dfe3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080a9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    # This dictionary is the first step. It takes the user's question string\n",
    "    # and prepares the 'context' and 'question' fields for the prompt.\n",
    "    {\"context\": retrieve_and_format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      "budgeting becomes a tool of empowerment. The mindset shifts from a reactive \"I can't spend on this\" to a proactive \"I am choosing not to spend on this, so I can achieve my goal of that.\"\n",
      "Time taken: 14.29 seconds\n",
      "----------------\n",
      "\n",
      "\n",
      "--- Answer ---\n",
      "i don't know\n",
      "Time taken: 2.11 seconds\n",
      "----------------\n",
      "\n",
      "\n",
      "--- Answer ---\n",
      "a notebook, a spreadsheet, or a dedicated app\n",
      "Time taken: 2.92 seconds\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interactive question-answering loop\n",
    "import time\n",
    "while True:\n",
    "    question = input(\"Ask a question about your documents (or type 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Invoke the RAG chain\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"\\n--- Quation ---\")\n",
    "    print(question)\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cad7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
